{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame transform with the Spark SQL ReAct Agent\n",
    "\n",
    "Use [Kaggle Subscription Dataset](https://www.kaggle.com/datasets/gsagar12/dspp1/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## install necessary packages\n",
    "# !pip install pyspark_ai\n",
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"subscription\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2021-01-01|\n",
      "|2021-01-02|\n",
      "|2021-01-03|\n",
      "|2021-01-04|\n",
      "|2021-01-05|\n",
      "|2021-01-06|\n",
      "|2021-01-07|\n",
      "|2021-01-08|\n",
      "|2021-01-09|\n",
      "|2021-01-10|\n",
      "|2021-01-11|\n",
      "|2021-01-12|\n",
      "|2021-01-13|\n",
      "|2021-01-14|\n",
      "|2021-01-15|\n",
      "|2021-01-16|\n",
      "|2021-01-17|\n",
      "|2021-01-18|\n",
      "|2021-01-19|\n",
      "|2021-01-20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate dates from start_date to end_date\n",
    "start_date = '2021-01-01'\n",
    "end_date = '2021-12-31'\n",
    "dates_query = f\"SELECT sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day) as date\"\n",
    "dates_df = spark.sql(dates_query).withColumn(\"date\", F.explode(F.col(\"date\")))\n",
    "dates_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(\"customer_product.csv\", header=True).createOrReplaceTempView(\"customer_product\")\n",
    "spark.read.csv(\"product_info.csv\", header=True).createOrReplaceTempView(\"product_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-----------------------------------+\n",
      "|min(CAST(signup_date_time AS DATE))|max(CAST(signup_date_time AS DATE))|\n",
      "+-----------------------------------+-----------------------------------+\n",
      "|                         2017-01-01|                         2021-12-31|\n",
      "+-----------------------------------+-----------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select min(CAST(signup_date_time as DATE)), max(CAST(signup_date_time as DATE)) from customer_product\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------------+----------------+--------------------+-----+-------------+\n",
      "|customer_id|product|signup_date_time|cancel_date_time|                name|price|billing_cycle|\n",
      "+-----------+-------+----------------+----------------+--------------------+-----+-------------+\n",
      "|      C2448|  prd_1|      2017-01-01|            NULL| annual_subscription| 1200|           12|\n",
      "|      C2449|  prd_1|      2017-01-01|      2021-09-05| annual_subscription| 1200|           12|\n",
      "|      C2450|  prd_1|      2017-01-01|      2019-01-13| annual_subscription| 1200|           12|\n",
      "|      C2451|  prd_2|      2017-01-01|            NULL|monthly_subscription|  125|            1|\n",
      "|      C2452|  prd_1|      2017-01-01|      2021-06-28| annual_subscription| 1200|           12|\n",
      "|      C2453|  prd_1|      2017-01-01|            NULL| annual_subscription| 1200|           12|\n",
      "|      C2454|  prd_1|      2017-01-01|      2021-06-01| annual_subscription| 1200|           12|\n",
      "|      C2455|  prd_1|      2017-01-02|            NULL| annual_subscription| 1200|           12|\n",
      "|      C2456|  prd_1|      2017-01-02|      2017-06-08| annual_subscription| 1200|           12|\n",
      "|      C2457|  prd_1|      2017-01-02|      2017-11-25| annual_subscription| 1200|           12|\n",
      "|      C2458|  prd_1|      2017-01-02|      2019-01-30| annual_subscription| 1200|           12|\n",
      "|      C2459|  prd_2|      2017-01-02|      2018-10-14|monthly_subscription|  125|            1|\n",
      "|      C2460|  prd_1|      2017-01-02|            NULL| annual_subscription| 1200|           12|\n",
      "|      C2461|  prd_1|      2017-01-02|            NULL| annual_subscription| 1200|           12|\n",
      "|      C2462|  prd_1|      2017-01-02|            NULL| annual_subscription| 1200|           12|\n",
      "|      C2463|  prd_1|      2017-01-02|      2019-02-13| annual_subscription| 1200|           12|\n",
      "|      C2464|  prd_1|      2017-01-02|            NULL| annual_subscription| 1200|           12|\n",
      "|      C2465|  prd_1|      2017-01-02|            NULL| annual_subscription| 1200|           12|\n",
      "|      C2466|  prd_1|      2017-01-02|            NULL| annual_subscription| 1200|           12|\n",
      "|      C2467|  prd_1|      2017-01-02|      2021-05-18| annual_subscription| 1200|           12|\n",
      "+-----------+-------+----------------+----------------+--------------------+-----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "sql_query = \"\"\"\n",
    "select c.customer_id, \n",
    "c.product, \n",
    "CAST(c.signup_date_time as DATE) as signup_date_time, \n",
    "CAST(c.cancel_date_time AS DATE) AS cancel_date_time,\n",
    "p.name,\n",
    "p.price,\n",
    "p.billing_cycle\n",
    "from customer_product c\n",
    "join product_info p on c.product = p.product_id\n",
    "\"\"\"\n",
    "df = spark.sql(sql_query)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Active Subscriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tradition sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+--------------------+------------------+\n",
      "|analysis_date|product|        product_name|subscription_count|\n",
      "+-------------+-------+--------------------+------------------+\n",
      "|   2021-10-05|  prd_1| annual_subscription|            252793|\n",
      "|   2021-10-05|  prd_2|monthly_subscription|            135700|\n",
      "+-------------+-------+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# calculate active subsription customers for a sepefic date and for each product\n",
    "df.createOrReplaceTempView(\"customer_product_combined\")\n",
    "analysis_date = '2021-10-05'\n",
    "subscritions_sql = f\"\"\"\n",
    "SELECT\n",
    "    '{analysis_date}' as analysis_date,\n",
    "    product,\n",
    "    name as product_name,\n",
    "    COUNT(distinct customer_id) AS subscription_count\n",
    "FROM\n",
    "    customer_product_combined\n",
    "WHERE\n",
    "    signup_date_time <= '{analysis_date}'\n",
    "    AND (cancel_date_time IS NULL OR cancel_date_time > '{analysis_date}')\n",
    "GROUP BY\n",
    "    1,2,3\n",
    "\"\"\"\n",
    "spark.sql(subscritions_sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generative ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark_ai import SparkAI\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.6)\n",
    "\n",
    "spark_ai = SparkAI(llm=llm,verbose=True)\n",
    "spark_ai.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO: \u001b[0mCreating temp view for the transform:\n",
      "df.createOrReplaceTempView(\u001b[33m\"\u001b[39;49;00m\u001b[33mspark_ai_temp_view_2ec336\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "**************text: Final Answer: \n",
      "\n",
      "SELECT product, COUNT(DISTINCT customer_id) AS active_subscription_customers\n",
      "FROM spark_ai_temp_view_2ec336\n",
      "WHERE signup_date_time <= '2021-10-05' AND (cancel_date_time > '2021-10-05' OR cancel_date_time IS NULL)\n",
      "GROUP BY product **************\n",
      "\u001b[32;1m\u001b[1;3mFinal Answer: \n",
      "\n",
      "SELECT product, COUNT(DISTINCT customer_id) AS active_subscription_customers\n",
      "FROM spark_ai_temp_view_2ec336\n",
      "WHERE signup_date_time <= '2021-10-05' AND (cancel_date_time > '2021-10-05' OR cancel_date_time IS NULL)\n",
      "GROUP BY product\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[92mINFO: \u001b[0mSQL query:\n",
      "\u001b[34mSELECT\u001b[39;49;00m\u001b[37m \u001b[39;49;00mproduct,\u001b[37m \u001b[39;49;00m\u001b[34mCOUNT\u001b[39;49;00m(\u001b[34mDISTINCT\u001b[39;49;00m\u001b[37m \u001b[39;49;00mcustomer_id)\u001b[37m \u001b[39;49;00m\u001b[34mAS\u001b[39;49;00m\u001b[37m \u001b[39;49;00mactive_subscription_customers\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mFROM\u001b[39;49;00m\u001b[37m \u001b[39;49;00mspark_ai_temp_view_2ec336\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mWHERE\u001b[39;49;00m\u001b[37m \u001b[39;49;00msignup_date_time\u001b[37m \u001b[39;49;00m<=\u001b[37m \u001b[39;49;00m\u001b[33m'2021-10-05'\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mAND\u001b[39;49;00m\u001b[37m \u001b[39;49;00m(cancel_date_time\u001b[37m \u001b[39;49;00m>\u001b[37m \u001b[39;49;00m\u001b[33m'2021-10-05'\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mOR\u001b[39;49;00m\u001b[37m \u001b[39;49;00mcancel_date_time\u001b[37m \u001b[39;49;00m\u001b[34mIS\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mNULL\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mGROUP\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mBY\u001b[39;49;00m\u001b[37m \u001b[39;49;00mproduct\u001b[37m\u001b[39;49;00m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------+\n",
      "|product|active_subscription_customers|\n",
      "+-------+-----------------------------+\n",
      "|  prd_1|                       252793|\n",
      "|  prd_2|                       135700|\n",
      "+-------+-----------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# active subscriptions at 2021-10-05\n",
    "df.ai.transform(\"calculate active subscription customers for each product at a specific date 2021-10-05\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Subscription Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tradition sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+--------------------+------------------+\n",
      "|analysis_date|product|        product_name|subscription_count|\n",
      "+-------------+-------+--------------------+------------------+\n",
      "|   2021-10-05|  prd_1| annual_subscription|               199|\n",
      "|   2021-10-05|  prd_2|monthly_subscription|               229|\n",
      "+-------------+-------+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# calculate new subsription customers for each product at 2021-10-05\n",
    "\n",
    "start_sql = f\"\"\"\n",
    "SELECT\n",
    "    '{analysis_date}' as analysis_date,\n",
    "    product,\n",
    "    name as product_name,\n",
    "    COUNT(distinct customer_id) AS subscription_count\n",
    "FROM\n",
    "    customer_product_combined\n",
    "WHERE\n",
    "    signup_date_time = '{analysis_date}'\n",
    "GROUP BY\n",
    "    1,2,3\n",
    "\"\"\"\n",
    "spark.sql(start_sql).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generative ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new subscription starts at 2021-10-05\n",
    "df.ai.transform(\"get new subscriptions for each product at date 2021-10-05\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Subscription Churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tradition sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate new subsription customers for each product at 2021-10-05\n",
    "\n",
    "start_sql = f\"\"\"\n",
    "SELECT\n",
    "    '{analysis_date}' as analysis_date,\n",
    "    product,\n",
    "    name as product_name,\n",
    "    COUNT(distinct customer_id) AS subscription_count\n",
    "FROM\n",
    "    customer_product_combined\n",
    "WHERE\n",
    "    cancel_date_time = '{analysis_date}'\n",
    "GROUP BY\n",
    "    1,2,3\n",
    "\"\"\"\n",
    "spark.sql(start_sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generative ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate how many customers churn for each product at 2021-10-05\n",
    "df.ai.transform(\"calculate how many customers churn for each product at 2021-10-05\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO: \u001b[0mCreating temp view for the transform:\n",
      "df.createOrReplaceTempView(\u001b[33m\"\u001b[39;49;00m\u001b[33mspark_ai_temp_view_703cd8\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "**************text: Thought: I will select the `customer_id`, `name`, and calculate the tenure using the `signup_date_time` and `cancel_date_time` columns. If `cancel_date_time` is null, I will use the current date. Then I will order the results by tenure in descending order and limit the output to the top 10 customers.\n",
      "\n",
      "Action: query_validation\n",
      "Action Input: \n",
      "SELECT customer_id, name, DATEDIFF(IF(cancel_date_time IS NULL, current_date, cancel_date_time), signup_date_time) AS tenure\n",
      "FROM spark_ai_temp_view_703cd8\n",
      "ORDER BY tenure DESC\n",
      "LIMIT 10\n",
      " **************\n",
      "\u001b[32;1m\u001b[1;3mThought: I will select the `customer_id`, `name`, and calculate the tenure using the `signup_date_time` and `cancel_date_time` columns. If `cancel_date_time` is null, I will use the current date. Then I will order the results by tenure in descending order and limit the output to the top 10 customers.\n",
      "\n",
      "Action: query_validation\n",
      "Action Input: \n",
      "SELECT customer_id, name, DATEDIFF(IF(cancel_date_time IS NULL, current_date, cancel_date_time), signup_date_time) AS tenure\n",
      "FROM spark_ai_temp_view_703cd8\n",
      "ORDER BY tenure DESC\n",
      "LIMIT 10\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mOK\u001b[0m\n",
      "Thought:**************text: I now know the final answer.\n",
      "Final Answer: \n",
      "SELECT customer_id, name, DATEDIFF(IF(cancel_date_time IS NULL, current_date, cancel_date_time), signup_date_time) AS tenure\n",
      "FROM spark_ai_temp_view_703cd8\n",
      "ORDER BY tenure DESC\n",
      "LIMIT 10 **************\n",
      "\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: \n",
      "SELECT customer_id, name, DATEDIFF(IF(cancel_date_time IS NULL, current_date, cancel_date_time), signup_date_time) AS tenure\n",
      "FROM spark_ai_temp_view_703cd8\n",
      "ORDER BY tenure DESC\n",
      "LIMIT 10\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[92mINFO: \u001b[0mSQL query:\n",
      "\u001b[34mSELECT\u001b[39;49;00m\u001b[37m \u001b[39;49;00mcustomer_id,\u001b[37m \u001b[39;49;00mname,\u001b[37m \u001b[39;49;00mDATEDIFF(\u001b[34mIF\u001b[39;49;00m(cancel_date_time\u001b[37m \u001b[39;49;00m\u001b[34mIS\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mNULL\u001b[39;49;00m,\u001b[37m \u001b[39;49;00m\u001b[34mcurrent_date\u001b[39;49;00m,\u001b[37m \u001b[39;49;00mcancel_date_time),\u001b[37m \u001b[39;49;00msignup_date_time)\u001b[37m \u001b[39;49;00m\u001b[34mAS\u001b[39;49;00m\u001b[37m \u001b[39;49;00mtenure\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mFROM\u001b[39;49;00m\u001b[37m \u001b[39;49;00mspark_ai_temp_view_703cd8\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mORDER\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34mBY\u001b[39;49;00m\u001b[37m \u001b[39;49;00mtenure\u001b[37m \u001b[39;49;00m\u001b[34mDESC\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mLIMIT\u001b[39;49;00m\u001b[37m \u001b[39;49;00m\u001b[34m10\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------+\n",
      "|customer_id|                name|tenure|\n",
      "+-----------+--------------------+------+\n",
      "|    C131975|monthly_subscription|  2480|\n",
      "|    C131994| annual_subscription|  2480|\n",
      "|    C131976| annual_subscription|  2480|\n",
      "|      C2451|monthly_subscription|  2480|\n",
      "|    C131977| annual_subscription|  2480|\n",
      "|    C131987| annual_subscription|  2480|\n",
      "|    C131991| annual_subscription|  2480|\n",
      "|    C131989| annual_subscription|  2480|\n",
      "|      C2453| annual_subscription|  2480|\n",
      "|    C131990| annual_subscription|  2480|\n",
      "+-----------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate how many customers churn for each product at 2021-10-05\n",
    "df.ai.transform(\"give me top 10 most tenure customers, if cancel_date_time is null, use current_date\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
